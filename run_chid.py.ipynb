{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset clue/chid to /home/mymusise/.cache/huggingface/datasets/clue/chid/1.0.0/e508b66266ba417d60e89ed8b167699cb4b56d3a2ead29b5667907d08069dbfc...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 139M/139M [00:24<00:00, 5.77MB/s] \n",
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset clue downloaded and prepared to /home/mymusise/.cache/huggingface/datasets/clue/chid/1.0.0/e508b66266ba417d60e89ed8b167699cb4b56d3a2ead29b5667907d08069dbfc. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 821.55it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"clue\", \"chid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {10: 84709})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "dataset['train'][0].keys()\n",
    "all_candidates = set()\n",
    "counter_cand_len = defaultdict(int)\n",
    "for example in dataset['train']:\n",
    "    for can in example['candidates']:\n",
    "        all_candidates.add(can)\n",
    "    counter_cand_len[len(example['candidates'])] += 1\n",
    "# dataset['train']['candidates']\n",
    "print(len(all_candidates))\n",
    "counter_cand_len\n",
    "\n",
    "candidates_ids = {candi: i for i, candi in enumerate(all_candidates)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'CpmTokenizer'. \n",
      "The class this function is called from is 'XLNetTokenizer'.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_path = \"/data2/CPM-distill/CPM-Generate-distill\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 5321/84709 [02:18<31:50, 41.56ex/s]  "
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "replace = re.compile('#idiom\\d{6}#')\n",
    "\n",
    "def preprocess(example):\n",
    "    features = []\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for ans_i, (candidate_id, ans) in enumerate(zip(example['answers']['candidate_id'], example['answers']['text'])):\n",
    "        for con_i, content in enumerate(example['content']):\n",
    "            sentence = replace.sub(ans, content)\n",
    "            if ans_i == con_i:\n",
    "                label = 1\n",
    "            else:\n",
    "                label = 0\n",
    "            # yield {'text': sentence, 'label': label}\n",
    "            sentences.append(sentence)\n",
    "            labels.append(label)\n",
    "            # features.append({'text': sentence, 'label': label})\n",
    "    features = tokenizer(sentences, max_length=256, truncation=True, padding=True, add_special_tokens=False, return_tensors='pt')\n",
    "    features['label'] = labels\n",
    "    return features\n",
    "    # return features\n",
    "\n",
    "\n",
    "# preprocess(dataset['train'][0])\n",
    "train_dataset = dataset['train'].map(preprocess)\n",
    "eval_dataset = dataset['validation'].map(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "labels = [0, 1]\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path, num_labels=len(labels)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure some model padding token id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", per_device_train_batch_size=16)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "print(f\"before train: {trainer.evaluate()}\")\n",
    "trainer.train()\n",
    "print(f\"after train: {trainer.evaluate()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0adcc2737ebf6a4a119f135174df96668767fca1ef1112612db5ecadf2b6d608"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
