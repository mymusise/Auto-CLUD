{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /home/mymusise/.cache/torch/sentence_transformers/mymusise_gpt2-medium-chinese. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /home/mymusise/.cache/torch/sentence_transformers/mymusise_gpt2-medium-chinese were not used when initializing GPT2Model: ['score.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.models import Transformer, Pooling\n",
    "from transformers import AutoTokenizer\n",
    "from typing import Union, List, Dict, Tuple\n",
    "\n",
    "model_path = \"mymusise/gpt2-medium-chinese\"\n",
    "\n",
    "\n",
    "model = SentenceTransformer(model_path)\n",
    "\n",
    "\n",
    "# for some tokenizer without cls_token in vocab\n",
    "if model.tokenizer.cls_token_id >= model.tokenizer.vocab_size:\n",
    "    model.tokenizer.cls_token = model.tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset clue (/home/mymusise/.cache/huggingface/datasets/clue/afqmc/1.0.0/e508b66266ba417d60e89ed8b167699cb4b56d3a2ead29b5667907d08069dbfc)\n",
      "100%|██████████| 3/3 [00:00<00:00, 1321.32it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"clue\", \"afqmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, SentencesDataset, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = DataLoader([InputExample(texts=[sample['sentence1'], sample['sentence2']], label=float(sample['label'])) for sample in dataset['train']], shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration: 100%|██████████| 1073/1073 [04:58<00:00,  3.59it/s]\n",
      "Iteration: 100%|██████████| 1073/1073 [04:59<00:00,  3.58it/s]\n",
      "Iteration: 100%|██████████| 1073/1073 [04:49<00:00,  3.70it/s]\n",
      "Epoch: 100%|██████████| 3/3 [14:48<00:00, 296.08s/it]\n"
     ]
    }
   ],
   "source": [
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "model.fit(train_objectives=[(train_dataset, train_loss)], epochs=3, warmup_steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4316it [00:00, 28798.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision=0.49, f1=0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, precision_score, f1_score\n",
    "from scipy.spatial.distance import cosine\n",
    "from tqdm import tqdm\n",
    "\n",
    "eva_input = []\n",
    "eva_y = []\n",
    "eva_y_pred = []\n",
    "\n",
    "test_set = dataset['validation']\n",
    "\n",
    "vec1s = model.encode(test_set['sentence1'])\n",
    "vec2s = model.encode(test_set['sentence2'])\n",
    "label = test_set['label']\n",
    "for v1, v2, l in tqdm(zip(vec1s, vec2s, label)):\n",
    "    dis = cosine(v1, v2)\n",
    "    sim = 1 - dis\n",
    "    eva_y_pred.append(1 if sim > 0.5 else 0)\n",
    "    # eva_input.append([sent1, sent2])\n",
    "    eva_y.append(l)\n",
    "\n",
    "precision = precision_score(eva_y, eva_y_pred)\n",
    "f1 = f1_score(eva_y, eva_y_pred)\n",
    "print(f\"{precision=:.03}, {f1=:.03}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
